{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "8c8642c8de97e1210f03becd08c8a019b275eb9152d23a17429f4f25663a4fde"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Fine tune BERT language model for sentiment analysis\n",
    "## BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is pre-trained language model, trained on 800M words and has 110M parameters  \n",
    "Using a larger model to improve the accuracy of sentiment analysis task, which was under 40% from LSTM model trained from scratch\n",
    "\n",
    "Fine tune the input and output layers to take in headlines news and classify them into negative, natural or positive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import official.nlp.bert.tokenization as tokenization\n",
    "\n",
    "from official import nlp\n",
    "import official.nlp.optimization as opt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "source": [
    "Load data, data from 'data_processor_catergoical.py'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_FB_catergorical.csv')"
   ]
  },
  {
   "source": [
    "Split data int training and testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into list for model inpuot\n",
    "x = df.Headlines.values\n",
    "y = df.Labels.values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "source": [
    "Adapted github user's adam0ling - twiter_sentiment \"https://github.com/adam0ling/twitter_sentiment/blob/main/3_BERT.ipynb\" and updated to version 3 of BERT multi cased\n",
    "\n",
    "Label Encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "encoder_y_train = encoder.transform(y_train)\n",
    "\n",
    "#Encode (0,1,2) labels into (001, 010, 100)\n",
    "dummy_y_test = np_utils.to_categorical(encoded_y_test)\n",
    "dummy_y_train = np_utils.to_categorical(encoder_y_train)"
   ]
  },
  {
   "source": [
    "Download Bert layers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\", #Using cased version, cased words in headlines news carries meaningful insights\n",
    "                            trainable=True)"
   ]
  },
  {
   "source": [
    "Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[101, 102]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])"
   ]
  },
  {
   "source": [
    "Function to tokenize input list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_names(n):\n",
    "    tokens = list(tokenizer.tokenize(n))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "headlines = tf.ragged.constant([encode_names(n) for n in x_train])\n",
    "\n",
    "cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*headlines.shape[0]\n",
    "input_word_ids = tf.concat([cls, headlines], axis=-1)\n",
    "\n",
    "input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "type_cls = tf.zeros_like(cls)\n",
    "type_headline = tf.ones_like(headlines)\n",
    "input_type_ids = tf.concat([type_cls, type_headline], axis=-1).to_tensor()\n",
    "\n",
    "lens = [len(i) for i in input_word_ids]\n",
    "max_seq_length = max(lens)\n",
    "max_seq_length = int(1.5*max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_names(n, tokenizer):\n",
    "    tokens = list(tokenizer.tokenize(n))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(string_list, tokenizer, max_seq_length):\n",
    "    num_examples = len(string_list)\n",
    "\n",
    "    string_tokens = tf.ragged.constant([\n",
    "        encode_names(n, tokenizer) for n in np. array(string_list)\n",
    "    ])\n",
    "\n",
    "    cls = [tokenizer.convert_tokens_to_ids('[CLS]')]*string_tokens.shape[0]\n",
    "    input_word_ids = tf.concat([cls, string_tokens], axis=-1)\n",
    "\n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_tokens = tf.ones_like(string_tokens)\n",
    "    input_type_ids = tf.concat(\n",
    "        [type_cls, type_tokens], axis=-1).to_tensor(shape=(None, max_seq_length))\n",
    "\n",
    "    inputs = {\n",
    "        'input_words_ids' : input_word_ids.to_tensor(shape=(None,  max_seq_length)),\n",
    "        'input_mask' : input_mask,\n",
    "        'input_type_ids' : input_type_ids\n",
    "    }\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "source": [
    "Tokenize training and testing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = bert_encode(x_train, tokenizer, max_seq_length)\n",
    "X_test = bert_encode(x_test, tokenizer, max_seq_length)"
   ]
  },
  {
   "source": [
    "Set sequence length and number of classes "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "num_class = len(encoder.classes_) #3 classes \n",
    "max_seq_length = max_seq_length"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "source": [
    "Fine tune model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer\n",
    "encoder_inputs = dict(\n",
    "    input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "    input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    "    input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
    ")\n",
    "\n",
    "outputs = bert_layer(encoder_inputs)\n",
    "\n",
    "pooled_output = outputs[\"pooled_output\"]\n",
    "sequence_output = outputs[\"sequence_output\"]\n",
    "\n",
    "output = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n",
    "output = tf.keras.layers.Dense(num_class, activation='softmax', name='output')(output) #Output layer\n",
    "\n",
    "model = tf.keras.Model(inputs=encoder_inputs, outputs=output) #Fine tuned model"
   ]
  },
  {
   "source": [
    "Model training parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 10 #Depend on your GPU ram, increase for faster training\n",
    "eval_batch_size = batch_size\n",
    "\n",
    "train_data_size = len(dummy_y_test)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "optimizer = nlp.optimization.create_optimizer(2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "source": [
    "Compile model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "Train model on 2 epochs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "213/213 [==============================] - 97s 457ms/step - loss: 1.0484 - accuracy: 0.4908 - val_loss: 0.9999 - val_accuracy: 0.5330\n",
      "Epoch 2/2\n",
      "213/213 [==============================] - 95s 447ms/step - loss: 1.0239 - accuracy: 0.5186 - val_loss: 0.9999 - val_accuracy: 0.5330\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    dummy_y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_test, dummy_y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "source": [
    "model.save('bert_headline.h5')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Conclusion\n",
    "BERT model outperformed LSTM model that was trained from scratch  \n",
    "BERT's validation accuracy is 53% compared to LSTM validation accruacy of 40%\n",
    "\n",
    "## Lack of training data\n",
    "Althought there were only 2,000 training samples, a larger data set could increase the accuracy at the cost of higher training time  \n",
    "Model was able to learn to classify better than random guessing, having a slight edge could be using in developing a strategy\n",
    "\n",
    "## To-do:\n",
    "* Collect more data, instead of collecting headline news of a specific stock  \n",
    "Collect data of the entire market and train it on an index eg. S&P 500  \n",
    "* Impelement a mid-low frequency trading strategy  \n",
    "Softmax activation funcation gives the probability of the output, probability could be used for risk management\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}